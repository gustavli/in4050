{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and logistic regression\n",
    "This week we will look at how linear regression may be used as a binary classifier. We will then consider the logistic regression classifier and compare the two.\n",
    "\n",
    "Then we will familiarize ourselves with  scikit-learn and make some comparisons to what we have done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will use simple synthetic data similarly to week_05, but we will make a little bigger sets to get more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X_train, y_train = make_blobs(n_samples=500, centers=[[0,0],[1,2]], \n",
    "                  n_features=2, random_state=2019)\n",
    "X_test, y_test = make_blobs(n_samples=500, centers=[[0,0],[1,2]], \n",
    "                  n_features=2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(X, y, marker='.'):\n",
    "    labels = set(y)\n",
    "    for lab in labels:\n",
    "        plt.plot(X[y == lab][:, 1], X[y == lab][:, 0],\n",
    "                 marker, label=\"class {}\".format(lab))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression classifier\n",
    "This is also called Ridge regression in the literature when it is smoothed. We will consider the simple unsmoothed version here and return to smoothing and regularization at a later lecture.\n",
    "\n",
    "We saw last week how we could use gradient descent to implement a simple linear regressor, which we repeat here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinReg():\n",
    "\n",
    "    def fit(self, X_train, t_train, gamma = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a Nxm matrix, N data points, m features\n",
    "        t_train are the targets values for training data\"\"\"\n",
    "        \n",
    "        (k, m) = X_train.shape\n",
    "        X_train = add_bias(X_train)\n",
    "        \n",
    "        self.theta = theta = np.zeros(m+1)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            theta -= gamma / k *  X_train.T @ (X_train @ theta - t_train)      \n",
    "    \n",
    "    def predict(self, x):\n",
    "        z = add_bias(x)\n",
    "        score = z @ self.theta\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "Make a linear regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "Train the classifier on X_train, y_train and calculate the accuracy on X_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic function\n",
    "First write code for the logistic function (sometimes called the sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    pass # fill in the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for the classifier\n",
    "Write code for the logistic regression classifier. Compared to the linear regression classifier, you have to make adaptations to both fit and predict taking the logistic into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your classifier goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First experiment\n",
    "Train the classifier on X_train, y_train, and calculate the accuracy on X_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Did you get better results than with the linear regression classifier? That does not necessarily have to be the case for this data set. But, if your result is much inferior to the linear regression classifier, the reason might be the parameter settings. Experiment with the parameter values for the learning rate and the number of epochs to get an optimal result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Implement a procedure for calculating a confusion matrix for a classifier and try it on one of the runs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "In this course, we implement many machine learning (ML) algorithms ourselves, to get a better understanding of how the ML algorithms work. After completing the course, you will have a stronger background for selecting tools and interpreting the results of your experiments.\n",
    "\n",
    "After the course, when you come to work on real ML tasks, you will normally not implement everything yourself from scratch, but instead rely on some toolboxes with ML algorithms. scikit-learn is one such toolbox. It contains all the algorithms we have considered so far. It also contains many other tools, like scalers and tools for preparing data. We have already made use of some of these tools, e.g. for making synthetic data sets.\n",
    "\n",
    "You can find a first introduction to scikit-learn here: https://scikit-learn.org/stable/getting_started.html, and you are advised to work through it. Let us now consider how learners are represented in scikit-learn. We first import the linear regressions classifier, caled ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_cl = RidgeClassifier()\n",
    "ridge_cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may calculate the accuracy using the score-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "Find out how to import logistic regression from scikit-learn and train and test it on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the results compare to the Ridge-classifier and to your own solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More classifiers\n",
    "Find out how you can import a *k*-nearest-neighbor classifier from sklearn, and train and test it on the same data for various classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
